# Buildspec runs in the build stage of your pipeline.
version: 0.2
phases:
  install:
    runtime-versions:
      docker: 18
      ruby: 2.6
    commands:
      - echo "cd into $CODEBUILD_SRC_DIR"
      - cd $CODEBUILD_SRC_DIR
      # Download the copilot linux binary.
      - wget {{.BinaryS3BucketPath}}/copilot-linux-{{.Version}}
      - mv ./copilot-linux-{{.Version}} ./copilot-linux
      - chmod +x ./copilot-linux
  build:
    commands:
      - echo "Run your tests"
      # - make test
  post_build:
    commands:
      - ls -l
      - export COLOR="false"
      # First, upgrade the cloudformation stack of every environment in the pipeline.
      - pipeline=$(cat $CODEBUILD_SRC_DIR/copilot/pipeline.yml | ruby -ryaml -rjson -e 'puts JSON.pretty_generate(YAML.load(ARGF))')
      - pl_envs=$(echo $pipeline | jq -r '.stages[].name')
      - >
        for pl_env in $pl_envs; do
          ./copilot-linux env upgrade -n $pl_env;
        done;
      # Find all the local services in the workspace.
      - svc_ls_result=$(./copilot-linux svc ls --local --json)
      - svc_list=$(echo $svc_ls_result | jq '.services')
      - >
        if [ ! "$svc_list" = null ]; then
          svcs=$(echo $svc_ls_result | jq -r '.services[].name');
        fi
      # Find all the local jobs in the workspace.
      - job_ls_result=$(./copilot-linux job ls --local --json)
      - job_list=$(echo $job_ls_result | jq '.jobs')
      - >
        if [ ! "$job_list" = null ]; then
          jobs=$(echo $job_ls_result | jq -r '.jobs[].name');
        fi
      # Raise error if no services or jobs are found.
      - >
        if [ "$svc_list" = null ] && [ "$job_list" = null ]; then
          echo "No services or jobs found for the pipeline to deploy. Please create at least one service or job and push the manifest to the remote." 1>&2;
          exit 1;
        fi
      # Generate the cloudformation templates.
      # The tag is the build ID but we replaced the colon ':' with a dash '-'.
      # We truncate the tag (from the front) to 128 characters, the limit for Docker tags
      # (https://docs.docker.com/engine/reference/commandline/tag/)
      # Check if the `svc package` commanded exited with a non-zero status. If so, echo error msg and exit.
      - >
        for env in $pl_envs; do
          tag=$(sed 's/:/-/g' <<<"${CODEBUILD_BUILD_ID##*:}-${env}" | rev | cut -c 1-128 | rev)
          for svc in $svcs; do
          ./copilot-linux svc package -n $svc -e $env --output-dir './infrastructure' --tag $tag;
          if [ $? -ne 0 ]; then
            echo "Cloudformation stack and config files were not generated. Please check build logs to see if there was a manifest validation error." 1>&2;
            exit 1;
          fi
          done;
          for job in $jobs; do
          ./copilot-linux job package -n $job -e $env --output-dir './infrastructure' --tag $tag;
          if [ $? -ne 0 ]; then
            echo "Cloudformation stack and config files were not generated. Please check build logs to see if there was a manifest validation error." 1>&2;
            exit 1;
          fi
          done;
        done;
      - ls -lah ./infrastructure
      # Concatenate jobs and services into one var for addons and env file
      - WORKLOADS=$(echo $jobs $svcs)
      # If addons exists, upload addons templates to each S3 bucket and write template URL to template config files.
      - |
        for workload in $WORKLOADS; do
          ADDONSFILE=./infrastructure/$workload.addons.stack.yml
          if [ -f "$ADDONSFILE" ]; then
            tmp=$(mktemp)
            timestamp=$(date +%s){{range $bucket := .ArtifactBuckets}}
            aws s3 cp "$ADDONSFILE" "s3://{{$bucket.BucketName}}/manual/$timestamp/$workload.addons.stack.yml";{{range $envName := $bucket.Environments}}
            jq --arg a "https://{{$bucket.BucketName}}.s3.{{$bucket.Region}}.amazonaws.com/manual/$timestamp/$workload.addons.stack.yml" '.Parameters.AddonsTemplateURL = $a' ./infrastructure/$workload-{{$envName}}.params.json > "$tmp" && mv "$tmp" ./infrastructure/$workload-{{$envName}}.params.json{{end}}{{end}}
          fi
        done;
      # If env file exists, upload the env file to each S3 bucket and write template URL to template config files.
      - |
        for workload in $WORKLOADS; do
          manifest=$(cat $CODEBUILD_SRC_DIR/copilot/$workload/manifest.yml | ruby -ryaml -rjson -e 'puts JSON.pretty_generate(YAML.load(ARGF))')
          type=$(echo $manifest | jq -r '.type')
          if [ "$type" = "Request-Driven Web Service" ]; then
            echo "skipping env file uploading for Request-Driven Web Service $workload";
            continue
          fi
          for env in $pl_envs; do
            env_files=$(echo $manifest | jq --arg env "$env" '{base_env_file: .env_file, env_env_file: (.environments? | .[$env]? | .env_file?) }')
            env_file=$(echo $env_files | jq -r 'if (.env_env_file == null) then .base_env_file else .env_env_file end')
            ENVFILE="./$env_file"
            if [ -f "$ENVFILE" ]; then{{range $bucket := .ArtifactBuckets}}{{range $envName := $bucket.Environments}}
              if [ $env = {{$envName}} ]; then
                tmp=$(mktemp)
                checksum=$(cat $ENVFILE | sha256sum | head -c 64)
                aws s3 cp "$ENVFILE" "s3://{{$bucket.BucketName}}/manual/$checksum/$env_file";
                jq --arg a "arn:$PARTITION:s3:::{{$bucket.BucketName}}/manual/$checksum/$env_file" '.Parameters.EnvFileARN = $a' ./infrastructure/$workload-{{$envName}}.params.json > "$tmp" && mv "$tmp" ./infrastructure/$workload-{{$envName}}.params.json
              fi{{end}}{{end}}
            fi
          done;
        done;
      # Build images
      # - For each manifest file:
      #   - Read the path to the Dockerfile by translating the YAML file into JSON.
      #   - Run docker build.
      #   - For each environment:
      #     - Retrieve the ECR repository.
      #     - Login and push the image.
      - >
        for workload in $WORKLOADS; do
          manifest=$(cat $CODEBUILD_SRC_DIR/copilot/$workload/manifest.yml | ruby -ryaml -rjson -e 'puts JSON.pretty_generate(YAML.load(ARGF))')
          for env in $pl_envs; do
            tag=$(sed 's/:/-/g' <<<"${CODEBUILD_BUILD_ID##*:}-${env}" | rev | cut -c 1-128 | rev)
            images=$(echo $manifest | jq --arg env "$env" '{base_image: .image, env_image: (.environments? | .[$env]? | .image? // {}) }')
            image=$(echo $images | jq 'if (.env_image | length == 0) then .base_image else (.base_image | del(.location)) * .env_image end')
            image_location=$(echo $image | jq '.location')
            if [ ! "$image_location" = null ]; then
              echo "skipping image building because location is provided as $image_location";
              continue
            fi
            base_dockerfile=$(echo $image | jq '.build')
            build_dockerfile=$(echo $image | jq -r '.build?.dockerfile? // ""')
            build_context=$(echo $image | jq -r '.build?.context? // ""')
            build_target=$(echo $image | jq -r '.build?.target? // ""')
            dockerfile_args=$(echo $image | jq '.build?.args? // "" | to_entries?')
            build_cache_from=$(echo $image | jq -r '.build?.cache_from? // ""')
            df_rel_path=$( echo $base_dockerfile | sed 's/"//g')
            if [ -n "$build_dockerfile" ]; then 
              df_rel_path=$build_dockerfile
            fi
            df_path=$df_rel_path
            df_dir_path=$(dirname "$df_path")
            if [ -n "$build_context" ]; then
              df_dir_path=$build_context
            fi
            platform_string=$(echo $manifest | jq -r '.platform // ""')
            platform_os=$(echo $manifest | jq -r '.platform?.osfamily? // ""')
            platform_arch=$(echo $manifest | jq -r '.platform?.architecture? // ""')
            if [ "$platform_os" -a "$platform_arch" ]; then
              platform_string="$platform_os/$platform_arch"
            fi
            build_args=
            if [ -n "$dockerfile_args" ]; then
              for arg in $(echo $dockerfile_args | jq -r '.[] | "\(.key)=\(.value)"'); do 
                build_args="$build_args--build-arg $arg "
              done
            fi
            if [ -n "$build_target" ]; then
              build_args="$build_args--target $build_target "
            fi
            if [ -n "$build_cache_from" ]; then
              for arg in $(echo $build_cache_from | jq -r '.[]'); do
                build_args="$build_args--cache-from $arg "
              done
            fi
            if [ -n "$platform_string" ]; then
              build_args="$build_args--platform $platform_string "
            fi
            echo "Name: $workload"
            echo "Relative Dockerfile path: $df_rel_path"
            echo "Docker build context: $df_dir_path"
            echo "Docker build args: $build_args"
            echo "Running command: docker build -t $workload:$tag $build_args-f $df_path $df_dir_path";
            docker build -t $workload:$tag $build_args-f $df_path $df_dir_path;
            image_id=$(docker images -q $workload:$tag);
            repo=$(cat $CODEBUILD_SRC_DIR/infrastructure/$workload-$env.params.json | jq -r '.Parameters.ContainerImage');
            region=$(echo $repo | cut -d'.' -f4);
            $(aws ecr get-login-password --region $region | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$region.amazonaws.com);
            docker tag $image_id $repo;
            docker push $repo;
          done;
        done;
artifacts:
  files:
    - "infrastructure/*"
